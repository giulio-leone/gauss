"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[82],{4881(e,s,t){t.r(s),t.d(s,{assets:()=>i,contentTitle:()=>c,default:()=>d,frontMatter:()=>o,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"plugins/evals","title":"EvalsPlugin","description":"Evaluation metrics collection with custom scorers","source":"@site/docs/plugins/evals.md","sourceDirName":"plugins","slug":"/plugins/evals","permalink":"/onegenui-deep-agents/docs/plugins/evals","draft":false,"unlisted":false,"editUrl":"https://github.com/giulio-leone/onegenui-deep-agents/tree/main/docs/docs/plugins/evals.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"EvalsPlugin","description":"Evaluation metrics collection with custom scorers"},"sidebar":"tutorialSidebar","previous":{"title":"VectorlessPlugin","permalink":"/onegenui-deep-agents/docs/plugins/vectorless"},"next":{"title":"Multi-Runtime Support","permalink":"/onegenui-deep-agents/docs/runtime/"}}');var r=t(4848),l=t(8453);const o={sidebar_position:6,title:"EvalsPlugin",description:"Evaluation metrics collection with custom scorers"},c="EvalsPlugin",i={},a=[{value:"Quick Start",id:"quick-start",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Custom Scorers",id:"custom-scorers",level:2},{value:"Examples",id:"examples",level:3},{value:"Collected Metrics",id:"collected-metrics",level:2},{value:"EvalResult",id:"evalresult",level:2},{value:"Accessing Results",id:"accessing-results",level:2},{value:"Persistence",id:"persistence",level:2},{value:"Hooks Used",id:"hooks-used",level:2}];function u(e){const s={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"evalsplugin",children:"EvalsPlugin"})}),"\n",(0,r.jsxs)(s.p,{children:["The ",(0,r.jsx)(s.code,{children:"EvalsPlugin"})," automatically collects evaluation metrics for every agent run \u2014 latency, token usage, tool call frequency, and custom scoring functions."]}),"\n",(0,r.jsx)(s.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:'import { DeepAgent, createEvalsPlugin } from "@giulio-leone/gaussflow-agent";\n\nconst evals = createEvalsPlugin({\n  persist: true,\n  scorers: [\n    {\n      name: "brevity",\n      score: (_prompt, output) => Math.min(output.length / 1000, 1),\n    },\n  ],\n  onEval: (result) => {\n    console.log(`Latency: ${result.metrics.latencyMs}ms`);\n    console.log(`Steps: ${result.metrics.stepCount}`);\n    console.log(`Tokens: ${result.metrics.tokenUsage.total}`);\n  },\n});\n\nconst agent = DeepAgent.create({\n  model: openai("gpt-4o"),\n  instructions: "You are a helpful assistant.",\n})\n  .use(evals)\n  .build();\n\nawait agent.run("Explain TypeScript generics.");\n\nconst lastResult = evals.getLastResult();\nconsole.log(lastResult?.metrics.customScores.brevity); // 0.0 - 1.0\n'})}),"\n",(0,r.jsx)(s.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:"interface EvalsPluginOptions {\n  scorers?: EvalScorer[];      // Custom scoring functions\n  persist?: boolean;           // Persist results via MemoryPort (default: false)\n  onEval?: (result: EvalResult) => void | Promise<void>;  // Callback per evaluation\n}\n"})}),"\n",(0,r.jsx)(s.h2,{id:"custom-scorers",children:"Custom Scorers"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:"interface EvalScorer {\n  readonly name: string;\n  score(prompt: string, output: string, metrics: EvalMetrics): Promise<number> | number;\n}\n"})}),"\n",(0,r.jsxs)(s.p,{children:["Scorers receive the original prompt, the agent's output, and the collected metrics. They return a numeric score. If a scorer throws, its score is recorded as ",(0,r.jsx)(s.code,{children:"-1"}),"."]}),"\n",(0,r.jsx)(s.h3,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:'const scorers: EvalScorer[] = [\n  // Score based on output length\n  { name: "length", score: (_, output) => Math.min(output.length / 1000, 1) },\n\n  // Score based on step efficiency\n  { name: "efficiency", score: (_, __, metrics) => 1 / (1 + metrics.stepCount) },\n\n  // Score based on whether output contains code\n  { name: "has-code", score: (_, output) => output.includes("```") ? 1 : 0 },\n];\n'})}),"\n",(0,r.jsx)(s.h2,{id:"collected-metrics",children:"Collected Metrics"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:"interface EvalMetrics {\n  latencyMs: number;                              // Total run time\n  stepCount: number;                              // Number of steps\n  toolCalls: Record<string, number>;              // Tool call counts by name\n  tokenUsage: {\n    prompt: number;                               // Input tokens\n    completion: number;                           // Output tokens\n    total: number;                                // Total tokens\n  };\n  customScores: Record<string, number>;           // Scorer results\n}\n"})}),"\n",(0,r.jsx)(s.h2,{id:"evalresult",children:"EvalResult"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:"interface EvalResult {\n  id: string;                 // Unique eval ID\n  sessionId: string;          // Agent session ID\n  prompt: string;             // Original prompt\n  output: string;             // Agent output\n  metrics: EvalMetrics;       // Collected metrics\n  createdAt: number;          // Timestamp\n}\n"})}),"\n",(0,r.jsx)(s.h2,{id:"accessing-results",children:"Accessing Results"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-typescript",children:"const allResults = evals.getResults();    // All collected results\nconst lastResult = evals.getLastResult(); // Most recent result\nevals.clearResults();                     // Clear collected results\n"})}),"\n",(0,r.jsx)(s.h2,{id:"persistence",children:"Persistence"}),"\n",(0,r.jsxs)(s.p,{children:["When ",(0,r.jsx)(s.code,{children:"persist: true"}),", eval results are saved to the agent's ",(0,r.jsx)(s.code,{children:"MemoryPort"})," as metadata with key ",(0,r.jsx)(s.code,{children:"eval:<id>"}),". This allows retrieval across sessions when using a persistent memory adapter like ",(0,r.jsx)(s.code,{children:"SupabaseMemoryAdapter"}),"."]}),"\n",(0,r.jsx)(s.h2,{id:"hooks-used",children:"Hooks Used"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:"Hook"}),(0,r.jsx)(s.th,{children:"Purpose"})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"beforeRun"})}),(0,r.jsx)(s.td,{children:"Records start time and prompt"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"afterTool"})}),(0,r.jsx)(s.td,{children:"Counts tool calls by name"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"afterRun"})}),(0,r.jsx)(s.td,{children:"Calculates metrics, runs scorers, persists results"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"onError"})}),(0,r.jsx)(s.td,{children:"Cleans up run state on errors"})]})]})]})]})}function d(e={}){const{wrapper:s}={...(0,l.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453(e,s,t){t.d(s,{R:()=>o,x:()=>c});var n=t(6540);const r={},l=n.createContext(r);function o(e){const s=n.useContext(l);return n.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function c(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(l.Provider,{value:s},e.children)}}}]);